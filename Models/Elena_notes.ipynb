{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI LAB REPORT ELENA NOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_HCC = pd.read_csv(\"raw_data/HCC1806_SmartS_MetaData.tsv\",delimiter=\"\\t\",engine='python',index_col=0)\n",
    "df_meta_MCF = pd.read_csv(\"raw_data/MCF7_SmartS_MetaData.tsv\",delimiter=\"\\t\",engine='python',index_col=0)\n",
    "print(\"Meta data dimensions for HCC1806:\", df_meta_HCC.shape)\n",
    "print(\"Meta data dimensions for MCF7:\", df_meta_MCF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_HCC.head(10)\n",
    "df_meta_HCC.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_MCF.head(10)\n",
    "df_meta_MCF.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the unfiltered data\n",
    "\n",
    "We import all the files needed for the entire project (even model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HCC cell line\n",
    "df_HCC_s_f = pd.read_csv(\"raw_data/HCC1806_SmartS_Filtered_Data.txt\", delimiter=\"\\ \",engine='python',index_col=0)\n",
    "df_HCC_s_f_n_test = pd.read_csv(\"raw_data/HCC1806_SmartS_Filtered_Normalised_3000_Data_test_anonim.txt\", delimiter=\"\\ \",engine='python',index_col=0)\n",
    "df_HCC_s_f_n_train = pd.read_csv(\"raw_data/HCC1806_SmartS_Filtered_Normalised_3000_Data_train.txt\", delimiter=\"\\ \",engine='python',index_col=0)\n",
    "df_HCC_s_uf = pd.read_csv(\"raw_data/HCC1806_SmartS_Unfiltered_Data.txt\", delimiter=\"\\ \",engine='python',index_col=0)\n",
    "\n",
    "#MCF cell line\n",
    "df_MCF_s_f = pd.read_csv(\"raw_data/MCF7_SmartS_Filtered_Data.txt\", delimiter=\"\\ \",engine='python',index_col=0)\n",
    "df_MCF_s_f_n_test = pd.read_csv(\"raw_data/MCF7_SmartS_Filtered_Normalised_3000_Data_test_anonim.txt\", delimiter=\"\\ \",engine='python',index_col=0)\n",
    "df_MCF_s_f_n_train = pd.read_csv(\"raw_data/MCF7_SmartS_Filtered_Normalised_3000_Data_train.txt\", delimiter=\"\\ \",engine='python',index_col=0)\n",
    "df_MCF_s_uf = pd.read_csv(\"raw_data/MCF7_SmartS_Unfiltered_Data.txt\", delimiter=\"\\ \",engine='python',index_col=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will first analyse the unfiltered data through plots and graphs. We will understand the dataset better, which will help us identify some potential problems of the dataset and give the foundation for the next steps, which will be filtering and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of genes for unfiltered MCF7 data: \", df_MCF_s_uf.shape[0])\n",
    "print(\"Number of cells for unfiltered MCF7 data: \", df_MCF_s_uf.shape[1])\n",
    "df_MCF_s_uf.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows of each dataset represent gene codes and are labeled as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 gene codes of HCC1806 data: \\n\", np.array(df_HCC_s_uf.index.values)[:5], \"\\n\")\n",
    "print(\"First 5 gene codes of MCF7 data:\\n \", np.array(df_MCF_s_uf.index.values)[:5])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the columns of the datasets are the individual cells that were sequenced. In the cell name itself, a lot of information is given: technique used, Hyp/Norm, (...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 cells of HCC1806 data: \\n\", np.array(df_HCC_s_uf.columns)[:5], \"\\n\")\n",
    "print(\"First 5 cells of MCF7 data:\\n \", np.array(df_MCF_s_uf.columns)[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! ADD !!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Values:\n",
    "One of the first things to check is whether there are missing values. In these datasets, there are none: this is due to the fact that if a gene was not found in a specific cell, the value was set to 0, eliminating the possibility of NA. We do notice, however, that many rows contain a large amount of zeros, which is a problem which we will discuss further on.\n",
    "Since there are no missing values, we can proceed by doing some preliminary analysis of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function which returns the number of missing values given a data set\n",
    "def missing(df):\n",
    "    miss = False\n",
    "    for c in df.columns:\n",
    "        if df[c].isnull().sum() != 0:\n",
    "            miss = True\n",
    "            return str(df[c].isnull().sum())\n",
    "    if not miss:\n",
    "        return \"No missing values\"\n",
    "\n",
    "print(\"Number of missing values for the HCC1806 data: \", missing(df_HCC_s_uf))\n",
    "print(\"Number of missing values for the MCF7 data: \", missing(df_MCF_s_uf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, there are many zero values, hence some genes occur rarely. This means that we are dealing with a sparse dataset, and it has to be taken into account throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that returns percentage of entries which are zero given a data frame\n",
    "def frac_zeros(df, n=20):\n",
    "    return (((df == 0).sum(axis=1).sum())/(df.shape[0] * df.shape[1])) * 100\n",
    "\n",
    "\n",
    "print(\"Percentage of entries which are zero in the HCC1806 dataset: \", f\"{round(frac_zeros(df_HCC_s_uf), 2)}%\")\n",
    "print(\"Percentage of entries which are zero in the MCF7 dataset: \", f\"{round(frac_zeros(df_MCF_s_uf), 2)}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOXPLOT, KERNEL DENSITY PLOT, HEATMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df_MCF_s_uf.values)\n",
    "plt.show()\n",
    "\n",
    "plt.boxplot(df_HCC_s_uf.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df_MCF_s_uf, shade=True)\n",
    "#plt.xlabel(\"\")\n",
    "#plt.ylabel(\"\")\n",
    "#plt.title(\"\")\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(df_HCC_s_uf, shade=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_MCF_s_uf.corr(), annot=True)\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(df_HCC_s_uf.corr(), annot=True)\n",
    "plt.show()\n",
    "\n",
    "#A heatmap is a graphical representation of data where the values are represented as colors in a two-dimensional matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swarm plot, Pair plot, Clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x=df[cnames[i**2]], data=df_MCF_s_uf)\n",
    "plt.show()\n",
    "\n",
    "sns.swarmplot(x=df[cnames[i**2]], data=df_HCC_s_uf)\n",
    "plt.show()\n",
    "\n",
    "#A swarm plot is a categorical scatter plot that displays the distribution of observations for each category by position along the horizontal axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_MCF_s_uf)\n",
    "plt.show()\n",
    "\n",
    "sns.pairplot(df_HCC_s_uf)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Pair plot: A pair plot is a grid of scatter plots that displays the pairwise relationships between variables in a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(df_MCF_s_uf)\n",
    "plt.show()\n",
    "\n",
    "sns.clustermap(df_HCC_s_uf)\n",
    "plt.show()\n",
    "\n",
    "#A clustermap is a heatmap that arranges the rows and columns of a dataset according to their similarity. Clustermaps are useful for identifying groups and clusters within a dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the genes:\n",
    "As a next step we plot some violin graphs. It is a statistical graphic that takes a cell as input and visualizes how many genes take a specific value in that cell's column.\n",
    "There is somewhat of a drawback: the number of genes sequenced for each cell can be any positive integer essencially between 0 and over 50 000.\n",
    "Therefore, it is very rare that many genes occur exactly the same amount of times and the result is that there are a lot of genes that occur 0 times, and all the other genes are spread out between 0 and the maximum. We do observe, however, that the number of gene occurences tend to be accumulated around lower values and only a few genes have very large number of occurences. \n",
    "(??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to crate the violin plots\n",
    "cnames_MCF = list(df_MCF_s_uf.columns)\n",
    "cnames_HCC = list(df_HCC_s_uf.columns)\n",
    "def violin(df, n=5):\n",
    "    cnames = list(df.columns)\n",
    "    for i in range(n):\n",
    "        #We square i just so that we do not only get the first few cells.\n",
    "        sns.boxplot(x=df[cnames[i**2]])\n",
    "        sns.violinplot(x=df[cnames[i**2]])\n",
    "        plt.show()\n",
    "\n",
    "#Violin plots for the HCC1806 dataset  \n",
    "violin(df_HCC_s_uf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare directly the violin plots for 50 cells. For the reasons mentioned above these plots show us the range of gene occurences for some columns of our dataset, however as we have seen the points on the violin graphs have a tendency to be more present around lower values. We also (temporarily) randomly mix around the columns so that we are not allways graphing the same 50 or so cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing violin plots for the HCC1806 dataset\n",
    "plt.figure(figsize=(16,4))\n",
    "plot=sns.violinplot(data=df_HCC_s_uf.sample(frac=1, axis = 'columns').iloc[:,:50],palette=\"Set3\",cut=0)\n",
    "plt.setp(plot.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing violin plots for the MCF7 dataset\n",
    "plt.figure(figsize=(16,4))\n",
    "plot=sns.violinplot(data=df_MCF_s_uf.sample(frac=1, axis = 'columns').iloc[:,:50],palette=\"Set3\",cut=0)\n",
    "plt.setp(plot.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving our attention to the genes we decided to plot a graph which illustrates the total number of times a gene occurs over all the cells. Then, we plotted the 50 genes with the largest number of occurences. In doing so we get to see if the dataset contains some genes that appear a lot and some that never appear or if the apperences are more evenly spred. For both datasets we see that after the initail spike with very common genes the bar graph smooths out. We also calculated how many total gene occurences we are neglecting by plotting only the 50 most common genes, and we realize that the remaining genes still represent a very large amount of gene detections(which we expect because of the large amount of genes in the datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representing how often a specific gene is found in a cell(I picked the 50 largest ones)\n",
    "largest_HCC = df_HCC_s_uf.sum(axis='columns').nlargest(50)\n",
    "remaining_HCC = df_HCC_s_uf.sum(axis='columns').sum() - df_HCC_s_uf.sum(axis='columns')[largest_HCC.index.values].sum()\n",
    "print(\"Number of remaining occurences:\", remaining_HCC)\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = largest_HCC.plot.bar(stacked = True, fontsize = 5)\n",
    "plt.xlabel('Genes')\n",
    "plt.ylabel('Number of occurences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representing how often a specific gene is found in a cell(I picked the 50 largest ones)\n",
    "largest_MCF = df_MCF_s_uf.sum(axis='columns').nlargest(50)\n",
    "remaining_MCF = df_MCF_s_uf.sum(axis='columns').sum() - df_MCF_s_uf.sum(axis='columns')[largest_MCF.index.values].sum()\n",
    "print(\"Number of remaining occurences:\", remaining_MCF)\n",
    "plt.figure(figsize=(12,6))\n",
    "ax = largest_MCF.plot.bar(fontsize = 5)\n",
    "plt.xlabel('Genes')\n",
    "plt.ylabel('Number of occurences')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step will be very useful later on: For each data set we differenciate between cells from the hypoxia experiment and cells from the normoxia experiment. We then create two subdatasets, one of which contains all the columns corresponding to hypoxia cells and the other containing only columns of normoxia cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that retruns lists of all cells that were part of the hypoxia and normoxia groups\n",
    "def hypo_and_norm(df):\n",
    "    hypo = []\n",
    "    norm = []\n",
    "    for cell in df.columns:\n",
    "        if \"Hypo\" in cell.split(\"_\") or \"Hypoxia\" in cell.split(\"_\"):\n",
    "            hypo.append(cell)\n",
    "        elif \"Norm\" in cell.split(\"_\") or \"Normoxia\" in cell.split(\"_\"):\n",
    "            norm.append(cell)\n",
    "        else:\n",
    "            print(\"Unkown:\", cell)\n",
    "    return (hypo, norm)\n",
    "\n",
    "#Data sets that contain only hypoxia cells\n",
    "df_MCF_hypo = df_MCF_s_uf[hypo_and_norm(df_MCF_s_uf)[0]]\n",
    "df_HCC_hypo = df_HCC_s_uf[hypo_and_norm(df_HCC_s_uf)[0]]\n",
    "\n",
    "#Data sets that contain only normoxia cells\n",
    "df_MCF_norm = df_MCF_s_uf[hypo_and_norm(df_MCF_s_uf)[1]]\n",
    "df_HCC_norm = df_HCC_s_uf[hypo_and_norm(df_HCC_s_uf)[1]]\n",
    "\n",
    "#How many hypoxia and how many normoxia are in each dataset\n",
    "print(\"Number of cells exposed to hypoxia for HCC1806 data: \", len(hypo_and_norm(df_HCC_s_uf)[0])) \n",
    "print(\"Number of cells exposed to normoxia for HCC1806 data: \", len(hypo_and_norm(df_HCC_s_uf)[1]))\n",
    "\n",
    "print(\"Number of cells exposed to hypoxia for MCF data: \", len(hypo_and_norm(df_MCF_s_uf)[0])) \n",
    "print(\"Number of cells exposed to normoxia for MCF data: \", len(hypo_and_norm(df_MCF_s_uf)[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In view of our final goal of this report, we looked at whether (??) even without a model, there were some genes that seem to be more present in the cells of the hypoxia enviorment or vice versa. To illustrate this did the following for both datasets: First we took only the colums with cell from the hypoxia experiment and summed them so we could see how often each gene was found in the cells that had little oxygen. We did the same for the Normoxia cells and we did the differences (in abs) between the gene occurences in normoxia cells and hypoxia cells. We took the 20 genes that had the largest differences. The idea of this represention is to see if some genes are obviously more present in hypoxia cells. If this was the case, we would be lead to believe that this gene may play a role in the survival of a cell with no oxygen. Similarly if a gene was very present only in normoxia cells then this gene might not be useful in a hypoxia enviorment(or it might even be degenerous). Please note that we cannot strongly conclude anything from the following graphs, differences might also be due to some sampling bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypo_vs_norm(df_hypo, df_norm,n=20, width = 0.25, title=\"Hypoxia vs Normoxia\"):\n",
    "    #Get a list of the total occurences of each gene\n",
    "    genes_norm = df_norm.sum(axis='columns')\n",
    "    genes_hypo = df_hypo.sum(axis='columns')\n",
    "\n",
    "    #Find the genes with the biggest difference of occurences between hypo cells and norm cells\n",
    "    largest_diffs = (genes_hypo.sub(genes_norm)).apply(abs).nlargest(n)\n",
    "    largest_diffs_genes = largest_diffs.index.values\n",
    "\n",
    "    #Bar graph with gene occurences in hypo vs norm\n",
    "    plt.bar(np.arange(len(genes_hypo[largest_diffs_genes])), \n",
    "            genes_hypo[largest_diffs_genes].tolist(), \n",
    "            color ='r', \n",
    "            width = width,\n",
    "            edgecolor ='grey', \n",
    "            label ='Hypoxia')\n",
    "\n",
    "    plt.bar([x + width for x in np.arange(len(genes_hypo[largest_diffs_genes]))],\n",
    "            genes_norm[largest_diffs_genes].tolist(), \n",
    "            color ='g', \n",
    "            width = width,\n",
    "            edgecolor ='grey', \n",
    "            label ='Normoxia')\n",
    "\n",
    "    plt.xticks([r + width for r in range(len(largest_diffs_genes))],\n",
    "            largest_diffs_genes,\n",
    "            rotation=90,\n",
    "            fontsize=10)\n",
    "    plt.title(title, weight='bold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return largest_diffs_genes\n",
    "\n",
    "\n",
    "hypo_vs_norm(df_HCC_hypo, df_HCC_norm, title = \"Genes with the largest difference of occurences for HCC\")\n",
    "hypo_vs_norm(df_MCF_hypo, df_MCF_norm, title = \"Genes with the largest difference of occurences for MCF\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning:\n",
    "Now that we have analysed the datasets, we can move on to data cleaning. In this process, we can identify and correct any potential issues that could arise when analysing our dataset. We had already foreshadowed this while checking for missing values, but since there were none we did not have to change the dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers:\n",
    "The next step is to find any possible outliers and remove them. Outliers will definitely degrade the performace of our models, and so we ought to remove them to avoid this problem. We must tread carefully though, because if we decide to remove a point we must be very confident that it is indeed an outlier and it does not contain any useful infomation, otherwise it can compromise the data and therefore our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_HCC = df_HCC_s_uf.quantile(0.25)\n",
    "Q3_HCC = df_HCC_s_uf.quantile(0.75)\n",
    "Q1_MCF = df_MCF_s_uf.quantile(0.25)\n",
    "Q3_MCF = df_MCF_s_uf.quantile(0.75)\n",
    "IQR_HCC = Q3_HCC - Q1_HCC\n",
    "IQR_MCF = Q3_MCF - Q1_MCF\n",
    "print(\"HCC:\\n\", IQR_HCC)\n",
    "print(\"MCF:\\n\", IQR_MCF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first and rather crude way of removing outliers is to only look at the quantiles. This is a very easy way of removing outliers however we risk eliminating a lot of useful data points. We see in fact that in both cases we have removed more than half of the cells when performing this operation. It is very improbable if not impossible that more than half of our data points are outliers. Later on we will see a better may to discover outliers, using clustering and SVMs for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HCC_noOut = df_HCC_s_uf[~((df_HCC_s_uf < (Q1_HCC - 1.5 * IQR_HCC)) |(df_HCC_s_uf > (Q3_HCC + 1.5 * IQR_HCC))).any(axis=1)]\n",
    "print(\"Shape with outliers: \", df_HCC_s_uf.shape)\n",
    "print(\"Shape without outliers: \", df_HCC_noOut.shape)\n",
    "print(\"Number of removed data points: \", df_HCC_s_uf.shape[0] - df_HCC_noOut.shape[0])\n",
    "df_HCC_noOut.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MCF_noOut = df_MCF_s_uf[~((df_MCF_s_uf < (Q1_MCF - 1.5 * IQR_MCF)) |(df_MCF_s_uf > (Q3_MCF + 1.5 * IQR_MCF))).any(axis=1)]\n",
    "print(\"Shape with outliers: \", df_MCF_s_uf.shape)\n",
    "print(\"Shape without outliers: \", df_MCF_noOut.shape)\n",
    "print(\"Number of removed data points: \", df_MCF_s_uf.shape[0] - df_MCF_noOut.shape[0])\n",
    "df_MCF_noOut.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most likely explanation for such a bad outlier detection using quantiles is that the data is very sparse. We have already seen that a lot of the entries of the dataset are zeros and so the quantiles are influenced dratically. This gives the impression that any data point with a lot of gene occurences is an outlier(which is indeed not the case), in fact these points are where most of our information comes from.\n",
    "\n",
    "words or counts of categorical data. On the other hand, features with dense data have predominantly non-zero values.\n",
    "\n",
    "can you quantify the sparsity?\n",
    "\n",
    "would using sparse matrix representation be an advantage?\n",
    "\n",
    "what would you do to adress this sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "###### Should we keep this, Matt does not really see the point\n",
    "######\n",
    "plt.figure(figsize=(16,4))\n",
    "plot=sns.violinplot(data=df_MCF_noOut.iloc[:,:50],palette=\"Set3\",cut=0)\n",
    "plt.setp(plot.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "###### Should we keep this, Matt does not really see the point\n",
    "######\n",
    "plt.figure(figsize=(16,4))\n",
    "plot=sns.violinplot(data=df_HCC_noOut.iloc[:,:50],palette=\"Set3\",cut=0)\n",
    "plt.setp(plot.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribiution:\n",
    "In any dataset it is always useful to at least have an idea of what kind of distribiution does your data follow. While we will never know with certainty we can find some models that best approximate the data. Let's start with the Skewness of both our datasets.\n",
    "\n",
    "From the graphs below we see that the data has very large positive skew. This is exactly what we expect from our dataset, indeed we metioned before that we have a lot of entries which are zero in out datasets, this will lead to the mode (of a single cell) to be probably 0. The mean on the other hand will be very afected by the large values present in our columns (which can reach the order of 1e5) and the median will be somewhere between the two. In fact the Fisher-Pearson Coefficient (cancluated by scripy.stats.skew) when mode < median < mean will return a positive number. In our case the skewness is very drastic as our numbers can range in a very large interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skewness\n",
    "from scipy.stats import skew\n",
    "\n",
    "def skewness(df1, df2, title1 = '', title2 = ''):\n",
    "  figure, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "  cnames1 = list(df1.columns)\n",
    "  cnames2 = list(df2.columns)\n",
    "  colN1 = np.shape(df1)[1]\n",
    "  colN2 = np.shape(df2)[1]\n",
    "  df_skew_cells1 = []\n",
    "  df_skew_cells2 = []\n",
    "\n",
    "  for i in range(colN1) :     \n",
    "      v_df1 = df1[cnames1[i]]\n",
    "      df_skew_cells1 += [skew(v_df1)]\n",
    "   \n",
    "  for i in range(colN2):\n",
    "     v_df2 = df2[cnames2[i]]\n",
    "     df_skew_cells2 += [skew(v_df2)]\n",
    "\n",
    "  #First graph \n",
    "  ax[0].hist(df_skew_cells1,bins=100)\n",
    "  ax[0].set_title(\"Skewness of single cells for \" + title1)\n",
    "\n",
    "  #Second graph \n",
    "  ax[1].hist(df_skew_cells2,bins=100)\n",
    "  ax[1].set_title(\"Skewness of single cells for \" + title2)\n",
    "  \n",
    "  #plt.xlabel('Skewness of single cells expression profiles - original df')\n",
    "  #print( \"Skewness of normal distribution: \", skew(df_skew_cells) )\n",
    "\n",
    "skewness(df_HCC_s_uf, df_MCF_s_uf, title1=\"HCC1806\", title2=\"MCF7\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kurtosis\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "def kurt(df1, df2, title1 = '', title2 = ''):\n",
    "  figure, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "  cnames1 = list(df1.columns)\n",
    "  cnames2 = list(df2.columns)\n",
    "  colN1 = np.shape(df1)[1]\n",
    "  colN2 = np.shape(df2)[1]\n",
    "  df_kurt_cells1 = []\n",
    "  df_kurt_cells2 = []\n",
    "\n",
    "  for i in range(colN1) :     \n",
    "      v_df1 = df1[cnames1[i]]\n",
    "      df_kurt_cells1 += [kurtosis(v_df1)]\n",
    "   \n",
    "  for i in range(colN2):\n",
    "     v_df2 = df2[cnames2[i]]\n",
    "     df_kurt_cells2 += [kurtosis(v_df2)]\n",
    "\n",
    "  #First graph \n",
    "  ax[0].hist(df_kurt_cells1,bins=100)\n",
    "  ax[0].set_title(\"Kurtosis of single cells for \" + title1)\n",
    "\n",
    "  #Second graph \n",
    "  ax[1].hist(df_kurt_cells2,bins=100)\n",
    "  ax[1].set_title(\"Kurtosis of single cells for \" + title2)\n",
    "  \n",
    "kurt(df_HCC_s_uf, df_MCF_s_uf, title1=\"HCC1806\", title2=\"MCF7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entropy\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def entro(df1, df2, title1 = '', title2 = ''):\n",
    "  figure, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "  cnames1 = list(df1.columns)\n",
    "  cnames2 = list(df2.columns)\n",
    "  colN1 = np.shape(df1)[1]\n",
    "  colN2 = np.shape(df2)[1]\n",
    "  df_kurt_cells1 = []\n",
    "  df_kurt_cells2 = []\n",
    "\n",
    "  for i in range(colN1) :     \n",
    "      v_df1 = df1[cnames1[i]]\n",
    "      df_kurt_cells1 += [entropy(v_df1)]\n",
    "   \n",
    "  for i in range(colN2):\n",
    "     v_df2 = df2[cnames2[i]]\n",
    "     df_kurt_cells2 += [entropy(v_df2)]\n",
    "\n",
    "  #First graph \n",
    "  ax[0].hist(df_kurt_cells1,bins=100)\n",
    "  ax[0].set_title(\"Entropy of single cells for \" + title1)\n",
    "\n",
    "  #Second graph \n",
    "  ax[1].hist(df_kurt_cells2,bins=100)\n",
    "  ax[1].set_title(\"Entropy of single cells for \" + title2)\n",
    "  \n",
    "entro(df_HCC_s_uf, df_MCF_s_uf, title1=\"HCC1806\", title2=\"MCF7\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution are highly non-normal, skewed with heavy tails. Why is this a problem?\n",
    "Problem because statistical tests assume model follows a normal distribution. If non-normal, results and estimates can be incorrect. We can fix this problem through data transformation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log transformation\n",
    "def transform_log2(df):\n",
    "    cnames = list(df.columns)\n",
    "    df_log2 = np.log2(df[cnames[1]]+1)\n",
    "    return df_log2\n",
    "\n",
    "\n",
    "sns.boxplot(x=transform_log2(df_MCF_s_uf))\n",
    "sns.violinplot(x=transform_log2(df_MCF_s_uf))\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(x=transform_log2(df_HCC_s_uf))\n",
    "sns.violinplot(x=transform_log2(df_HCC_s_uf))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plot=sns.violinplot(data=transform_log2(df_MCF_s_uf),palette=\"Set3\",cut=0)\n",
    "plt.setp(plot.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plot=sns.violinplot(data=transform_log2(df_HCC_s_uf),palette=\"Set3\",cut=0)\n",
    "plt.setp(plot.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_normalization(df):\n",
    "    df_small = df.iloc[:, 10:30]  #just selecting part of the samples so run time not too long\n",
    "    sns.displot(data=df_small,palette=\"Set3\",kind=\"kde\", bw_adjust=2)\n",
    "    plt.show()\n",
    "\n",
    "graph_normalization(df_MCF_s_uf)\n",
    "graph_normalization(df_MCF_s_f_n_train)\n",
    "\n",
    "graph_normalization(df_HCC_s_uf)\n",
    "graph_normalization(df_HCC_s_f_n_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_rows(df, all_cells = False, shape = False):\n",
    "    if shape:\n",
    "        print(\"Number of duplicate rows: \", df[df.duplicated(keep=False)].shape)\n",
    "    if all_cells:\n",
    "        print(\"Duplicate rows: \", df[df.duplicated(keep=False)])\n",
    "    return df[df.duplicated(keep=False)]\n",
    "\n",
    "duplicate_rows(df_MCF_s_uf)\n",
    "duplicate_rows(df_MCF_s_uf, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check where the duplicates are:\n",
    "\n",
    "duplicate_rows_df_MCF_t = duplicate_rows(df_MCF_s_uf).T\n",
    "c_dupl_MCF = duplicate_rows_df_MCF_t.corr()\n",
    "c_dupl_MCF\n",
    "\n",
    "duplicate_rows_df_HCC_t = duplicate_rows(df_HCC_s_uf).T\n",
    "c_dupl_HCC = duplicate_rows_df_HCC_t.corr()\n",
    "c_dupl_HCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning: the scatter plots below might take a long time if the number of duplicate features is large\n",
    "# sns.pairplot(duplicate_rows_df_MCF_t)\n",
    "# sns.pairplot(duplicate_rows_df_HCC_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the statistics of the gene expression profiles of genes/features that seem duplicates. They might be features with many zeros, or many missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows_df_MCF_t.describe()\n",
    "duplicate_rows_df_HCC_t.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicates\n",
    "\n",
    "df_MCF_noDup = df_MCF_s_uf.drop_duplicates()\n",
    "df_HCC_noDup = df_HCC_s_uf.drop_duplicates()\n",
    "\n",
    "print(df_MCF_noDup.count())\n",
    "print(df_HCC_noDup.count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "c_MCF = df_MCF_s_uf.corr()\n",
    "midpoint = (c_MCF.values.max() - c_MCF.values.min()) /2 + c_MCF.values.min()\n",
    "# sns.heatmap(c,cmap='coolwarm',annot=True, center=midpoint )\n",
    "# plt.show()\n",
    "sns.heatmap(c_MCF,cmap='coolwarm', center=0 )\n",
    "plt.show()\n",
    "print(\"Number of cells included: \", np.shape(c_MCF))\n",
    "print(\"Average correlation of expression profiles between cells: \", midpoint)\n",
    "print(\"Min. correlation of expression profiles between cells: \", c_MCF.values.min())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "c_HCC= df_HCC_s_uf.corr()\n",
    "midpoint = (c_HCC.values.max() - c_HCC.values.min()) /2 + c_HCC.values.min()\n",
    "#sns.heatmap(c,cmap='coolwarm',annot=True, center=midpoint )\n",
    "#plt.show()\n",
    "sns.heatmap(c_HCC,cmap='coolwarm', center=0 )\n",
    "plt.show()\n",
    "print(\"Number of cells included: \", np.shape(c_HCC))\n",
    "print(\"Average correlation of expression profiles between cells: \", midpoint)\n",
    "print(\"Min. correlation of expression profiles between cells: \", c_HCC.values.min())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could look at the distribution of the correlation between gene expression profiles using Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_small = c_MCF.iloc[:,:3]\n",
    "print(c_small)\n",
    "sns.histplot(c_small,bins=100)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Correlation between cells expression profiles')\n",
    "\n",
    "c_small = c_HCC.iloc[:,:3]\n",
    "sns.histplot(c_small,bins=100)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Correlation between cells expression profiles')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the correlation between the gene expression profiles of the the single cells to be fairly high.\n",
    "\n",
    "Some genes will be characteristic of some cells. For example in our case we expect some genes to be expressed at high levels only in cells cultured in conditions of low oxygen (hypoxia), or viceversa. However, most of the low and/or high expressed genes will tend to be generally similar. Several genes will have a high expression across cells as they are house keeping genes needed for the basic functioning of the cell. Some genes will have low expression across cells as they are less or not essential for the normal functioning, so they will have low or no expression across cells and will only be expressed in specific circumstances.\n",
    "\n",
    "Are there some cells which are not correlated with the others?\n",
    "\n",
    "Can you explore the distributions of gene expression for these cells and check why? Do they have more zero values than other cells?\n",
    "\n",
    "Or do they have higher values?\n",
    "\n",
    "Next you could explore the features/genes. Are they correlated? Is this expected? Could this generate issues in the ML?\n",
    "\n",
    "Repeat the steps above for all datasets, and discuss the findings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
