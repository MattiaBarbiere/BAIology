{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Prediction Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read the confusion matrics in the .csv files and store them in a pandas dataframe. Then, we will define a function to obtain the metrics such as accuracy, precision, recall etc for all the 4 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1405</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>2098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  1405   119\n",
       "1    49  2098"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropseq\n",
    "HCC_drop_pr_df = pd.read_csv(\"HCC1806_drop_table_pr.csv\")\n",
    "MCF_drop_pr_df = pd.read_csv(\"MCF7_drop_table_pr.csv\")\n",
    "\n",
    "#Smartseq\n",
    "\n",
    "HCC_smart_pr_df = pd.read_csv(\"HCC1806_smart_table_pr.csv\")\n",
    "MCF_smart_pr_df = pd.read_csv(\"MCF7_smart_table_pr.csv\")\n",
    "\n",
    "HCC_drop_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3170</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>2122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0  3170    69\n",
       "1    45  2122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCF_drop_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  25   0\n",
       "1   1  19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HCC_smart_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  32   0\n",
       "1   0  31"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MCF_smart_pr_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few remarks on the Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: Precision measures the proportion of correctly predicted positive instances (true positives) out of all instances predicted as positive (true positives + false positives). It indicates how reliable the positive predictions are. \n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the proportion of correctly predicted positive instances (true positives) out of all actual positive instances (true positives + false negatives). It indicates how effectively the model can identify positive instances.\n",
    "\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure between precision and recall. F1-score is useful when there is an imbalance between the positive and negative classes.\n",
    "\n",
    "Support: Support represents the number of instances in each class. It indicates the number of actual occurrences of the class in the dataset.\n",
    "\n",
    "Accuracy: Accuracy measures the proportion of correctly predicted instances (true positives + true negatives) out of all instances. It provides an overall performance measure of the model.\n",
    "\n",
    "False Positive Rate (FPR): FPR calculates the proportion of incorrectly predicted negative instances (false positives) out of all actual negative instances (true negatives + false positives). It indicates the rate of falsely identifying negative instances as positive. This can be calculated using the formula 1 - Recall of Class 0.\n",
    "\n",
    "False Negative Rate (FNR): FNR calculates the proportion of incorrectly predicted positive instances (false negatives) out of all actual positive instances (true positives + false negatives). It indicates the rate of falsely identifying positive instances as negative. This can be calculated using the formula 1 - Recall of Class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(confusion_matrix_df):\n",
    "    # Convert pandas DataFrame to numpy array\n",
    "    confusion_matrix = confusion_matrix_df.to_numpy()\n",
    "\n",
    "    # Check if the confusion matrix has the correct shape\n",
    "    if confusion_matrix.shape != (2, 2):\n",
    "        raise ValueError(\"Confusion matrix must be a 2x2 matrix.\")\n",
    "    \n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "    TP = confusion_matrix[1, 1]\n",
    "\n",
    "    # Calculate metrics for negative class (class 0)\n",
    "    precision_0 = TN / (TN + FN)\n",
    "    recall_0 = TN / (TN + FP)\n",
    "    f1_score_0 = 2 * (precision_0 * recall_0) / (precision_0 + recall_0)\n",
    "    support_0 = TN + FP\n",
    "\n",
    "    # Calculate metrics for positive class (class 1)\n",
    "    precision_1 = TP / (TP + FP)\n",
    "    recall_1 = TP / (TP + FN)\n",
    "    f1_score_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1)\n",
    "    support_1 = TP + FN\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    # Calculate false positive rate (FPR)\n",
    "    FPR = 1 - recall_0\n",
    "\n",
    "    # Calculate false negative rate (FNR)\n",
    "    FNR = 1 - recall_1\n",
    "\n",
    "    # Create a dictionary to store the metrics\n",
    "    metrics = {\n",
    "        'precision': [precision_0, precision_1],\n",
    "        'recall': [recall_0, recall_1],\n",
    "        'f1-score': [f1_score_0, f1_score_1],\n",
    "        'support': [support_0, support_1]\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame for the metrics\n",
    "    metrics_df = pd.DataFrame(metrics, index=['Negative (class 0)', 'Positive (class 1)'])\n",
    "\n",
    "    return metrics_df, accuracy, FPR, FNR\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropseq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision  recall  f1-score  support\n",
      "Negative (class 0)     0.9663  0.9219    0.9436     1524\n",
      "Positive (class 1)     0.9463  0.9772    0.9615     2147\n",
      "Accuracy: 95.42%\n",
      "False Positive Rate (FPR): 7.81%\n",
      "False Negative Rate (FNR): 2.28%\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_df = HCC_drop_pr_df\n",
    "metrics_df, accuracy, FPR, FNR = calculate_classification_metrics(confusion_matrix_df)\n",
    "\n",
    "# Print the classification report-style output\n",
    "print(metrics_df.to_string(float_format='%.4f'))\n",
    "\n",
    "# Print overall accuracy\n",
    "accuracy_percentage = accuracy * 100\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_percentage))\n",
    "\n",
    "# Print false positive rate and false negative rate\n",
    "FPR_percentage = FPR * 100\n",
    "FNR_percentage = FNR * 100\n",
    "print(\"False Positive Rate (FPR): {:.2f}%\".format(FPR_percentage))\n",
    "print(\"False Negative Rate (FNR): {:.2f}%\".format(FNR_percentage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision  recall  f1-score  support\n",
      "Negative (class 0)     0.9860  0.9787    0.9823     3239\n",
      "Positive (class 1)     0.9685  0.9792    0.9738     2167\n",
      "Accuracy: 97.89%\n",
      "False Positive Rate (FPR): 2.13%\n",
      "False Negative Rate (FNR): 2.08%\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_df = MCF_drop_pr_df\n",
    "metrics_df, accuracy, FPR, FNR = calculate_classification_metrics(confusion_matrix_df)\n",
    "\n",
    "# Print the classification report-style output\n",
    "print(metrics_df.to_string(float_format='%.4f'))\n",
    "\n",
    "# Print overall accuracy\n",
    "accuracy_percentage = accuracy * 100\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_percentage))\n",
    "\n",
    "# Print false positive rate and false negative rate\n",
    "FPR_percentage = FPR * 100\n",
    "FNR_percentage = FNR * 100\n",
    "print(\"False Positive Rate (FPR): {:.2f}%\".format(FPR_percentage))\n",
    "print(\"False Negative Rate (FNR): {:.2f}%\".format(FNR_percentage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SmartSeq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision  recall  f1-score  support\n",
      "Negative (class 0)     0.9615  1.0000    0.9804       25\n",
      "Positive (class 1)     1.0000  0.9500    0.9744       20\n",
      "Accuracy: 97.78%\n",
      "False Positive Rate (FPR): 0.00%\n",
      "False Negative Rate (FNR): 5.00%\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_df = HCC_smart_pr_df\n",
    "metrics_df, accuracy, FPR, FNR = calculate_classification_metrics(confusion_matrix_df)\n",
    "\n",
    "# Print the classification report-style output\n",
    "print(metrics_df.to_string(float_format='%.4f'))\n",
    "\n",
    "# Print overall accuracy\n",
    "accuracy_percentage = accuracy * 100\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_percentage))\n",
    "\n",
    "# Print false positive rate and false negative rate\n",
    "FPR_percentage = FPR * 100\n",
    "FNR_percentage = FNR * 100\n",
    "print(\"False Positive Rate (FPR): {:.2f}%\".format(FPR_percentage))\n",
    "print(\"False Negative Rate (FNR): {:.2f}%\".format(FNR_percentage))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision  recall  f1-score  support\n",
      "Negative (class 0)     1.0000  1.0000    1.0000       32\n",
      "Positive (class 1)     1.0000  1.0000    1.0000       31\n",
      "Accuracy: 100.00%\n",
      "False Positive Rate (FPR): 0.00%\n",
      "False Negative Rate (FNR): 0.00%\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_df = MCF_smart_pr_df\n",
    "metrics_df, accuracy, FPR, FNR = calculate_classification_metrics(confusion_matrix_df)\n",
    "\n",
    "# Print the classification report-style output\n",
    "print(metrics_df.to_string(float_format='%.4f'))\n",
    "\n",
    "# Print overall accuracy\n",
    "accuracy_percentage = accuracy * 100\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_percentage))\n",
    "\n",
    "# Print false positive rate and false negative rate\n",
    "FPR_percentage = FPR * 100\n",
    "FNR_percentage = FNR * 100\n",
    "print(\"False Positive Rate (FPR): {:.2f}%\".format(FPR_percentage))\n",
    "print(\"False Negative Rate (FNR): {:.2f}%\".format(FNR_percentage))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
